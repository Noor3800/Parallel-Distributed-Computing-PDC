{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "SP23-BAI-046\n",
        "Noor Fatima"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "esQIuzr5OvuJ"
      },
      "source": [
        "**imports & GPU check**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F7CNetuKM9Gr",
        "outputId": "5aa72d36-49fb-4b11-80f5-85bedb215e25"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Numba: 2.0.2\n",
            "CUDA available: True\n",
            "CUDA device name: b'Tesla T4'\n",
            "Compute capability: (7, 5)\n",
            "Max threads per block: 1024\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import numpy as np\n",
        "import time\n",
        "from numba import njit, prange\n",
        "from numba import cuda\n",
        "from PIL import Image\n",
        "import io\n",
        "import sys\n",
        "\n",
        "print(\"Numba:\", np.__version__)\n",
        "print(\"CUDA available:\", cuda.is_available())\n",
        "if cuda.is_available():\n",
        "    dev = cuda.get_current_device()\n",
        "    print(\"CUDA device name:\", dev.name)\n",
        "    print(\"Compute capability:\", dev.compute_capability)\n",
        "    print(\"Max threads per block:\", dev.MAX_THREADS_PER_BLOCK)\n",
        "else:\n",
        "    print(\"No CUDA device found. Please enable GPU in Colab: Runtime -> Change runtime type -> GPU\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LEy47E0NO2I5"
      },
      "source": [
        "**Hello GPU (threads / blocks / grids)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WdwxvBVoNBFl",
        "outputId": "df2ce21c-d96b-410f-d9c1-6c34da15e5eb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hello from thread 0\n",
            "Hello from thread 1\n",
            "Hello from thread 2\n",
            "Hello from thread 3\n",
            "Hello from thread 4\n",
            "Hello from thread 5\n",
            "Hello from thread 6\n",
            "Hello from thread 7\n",
            "Hello from thread 8\n",
            "Hello from thread 9\n",
            "Hello from thread 10\n",
            "Hello from thread 11\n",
            "Hello from thread 12\n",
            "Hello from thread 13\n",
            "Hello from thread 14\n",
            "Hello from thread 15\n"
          ]
        }
      ],
      "source": [
        "import cupy as cp\n",
        "\n",
        "# Kernel writes each thread's global id into the output array\n",
        "hello_kernel = cp.ElementwiseKernel(\n",
        "    '',\n",
        "    'int32 tid',\n",
        "    'tid = i;',  # i is the built-in thread/global index for elementwise\n",
        "    'hello_kernel'\n",
        ")\n",
        "\n",
        "n = 16\n",
        "out = cp.zeros(n, dtype=cp.int32)\n",
        "hello_kernel(out)\n",
        "\n",
        "# Bring results back to host\n",
        "out_host = cp.asnumpy(out)\n",
        "for idx, tid in enumerate(out_host):\n",
        "    print(f\"Hello from thread {tid}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "heHb0Y1FOT53"
      },
      "source": [
        "**Vector Addition (CPU vs GPU with CuPy)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uS0VlOYfN5lq",
        "outputId": "4eed37c1-8a1f-40a3-81b7-b6f6a011317a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Allocating arrays of length: 10000000\n",
            "CPU (NumPy) time: 0.0156 s\n",
            "GPU kernel time (CuPy): 0.0009 s\n",
            "Total GPU time (incl transfers): 0.0428 s\n",
            "Speedup (CPU / GPU kernel-only): 17.90x\n",
            "Speedup (CPU / GPU end-to-end): 0.36x\n",
            "Results identical: True\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import cupy as cp\n",
        "import time\n",
        "\n",
        "# Problem size\n",
        "N = 10_000_000\n",
        "print(\"Allocating arrays of length:\", N)\n",
        "\n",
        "# CPU arrays\n",
        "a_cpu = np.random.rand(N).astype(np.float32)\n",
        "b_cpu = np.random.rand(N).astype(np.float32)\n",
        "\n",
        "# ---------------- CPU baseline ----------------\n",
        "t0 = time.perf_counter()\n",
        "out_cpu = a_cpu + b_cpu   # NumPy vectorized (uses optimized C under the hood)\n",
        "t1 = time.perf_counter()\n",
        "cpu_time = t1 - t0\n",
        "print(f\"CPU (NumPy) time: {cpu_time:.4f} s\")\n",
        "\n",
        "# ---------------- GPU (CuPy) ----------------\n",
        "a_gpu = cp.asarray(a_cpu)\n",
        "b_gpu = cp.asarray(b_cpu)\n",
        "\n",
        "# warm-up\n",
        "_ = a_gpu + b_gpu\n",
        "cp.cuda.Stream.null.synchronize()\n",
        "\n",
        "# kernel timing only (data already on GPU)\n",
        "t0 = time.perf_counter()\n",
        "out_gpu = a_gpu + b_gpu\n",
        "cp.cuda.Stream.null.synchronize()\n",
        "t1 = time.perf_counter()\n",
        "gpu_kernel_time = t1 - t0\n",
        "print(f\"GPU kernel time (CuPy): {gpu_kernel_time:.4f} s\")\n",
        "\n",
        "# end-to-end timing (including H2D + compute + D2H)\n",
        "t0 = time.perf_counter()\n",
        "out_gpu_full = cp.asnumpy(cp.asarray(a_cpu) + cp.asarray(b_cpu))\n",
        "t1 = time.perf_counter()\n",
        "gpu_total_time = t1 - t0\n",
        "print(f\"Total GPU time (incl transfers): {gpu_total_time:.4f} s\")\n",
        "\n",
        "# ---------------- Speedup ----------------\n",
        "speedup_kernel = cpu_time / gpu_kernel_time if gpu_kernel_time > 0 else np.inf\n",
        "speedup_total  = cpu_time / gpu_total_time if gpu_total_time > 0 else np.inf\n",
        "print(f\"Speedup (CPU / GPU kernel-only): {speedup_kernel:.2f}x\")\n",
        "print(f\"Speedup (CPU / GPU end-to-end): {speedup_total:.2f}x\")\n",
        "\n",
        "# ---------------- Verify correctness ----------------\n",
        "print(\"Results identical:\", np.allclose(out_cpu, cp.asnumpy(out_gpu), atol=1e-6))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q45Zr7P1Og2N"
      },
      "source": [
        "**Image Inversion (CPU vs GPU with CuPy)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zIEgdjBqN9AS",
        "outputId": "31c17c12-f341-4162-c0b9-3d1f984828d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved fallback image: sample_gradient.png\n",
            "Image shape: (1024, 2048, 3)\n",
            "CPU invert time: 0.0029 s\n",
            "GPU invert time: 0.1445 s\n",
            "Outputs identical: True\n",
            "Saved inverted_cpu.png and inverted_gpu.png\n"
          ]
        }
      ],
      "source": [
        "from PIL import Image\n",
        "import numpy as np\n",
        "import cupy as cp\n",
        "import time\n",
        "import os\n",
        "\n",
        "# Try to load an uploaded image; else fallback to gradient\n",
        "try:\n",
        "    fname = \"sample.png\"  # change this if you upload your own\n",
        "    if not os.path.exists(fname):\n",
        "        raise FileNotFoundError\n",
        "except:\n",
        "    # fallback: create gradient image\n",
        "    W, H = 2048, 1024\n",
        "    arr = np.zeros((H, W, 3), dtype=np.uint8)\n",
        "    for y in range(H):\n",
        "        arr[y, :, :] = np.linspace(0, 255, W, dtype=np.uint8)[:, None].repeat(3, axis=1)\n",
        "    img = Image.fromarray(arr)\n",
        "    fname = \"sample_gradient.png\"\n",
        "    img.save(fname)\n",
        "    print(\"Saved fallback image:\", fname)\n",
        "\n",
        "# Load image\n",
        "img = Image.open(fname).convert(\"RGB\")\n",
        "arr = np.array(img)  # shape (H, W, 3), dtype=uint8\n",
        "print(\"Image shape:\", arr.shape)\n",
        "\n",
        "# ---------------- CPU inversion ----------------\n",
        "t0 = time.perf_counter()\n",
        "inv_cpu = 255 - arr\n",
        "t1 = time.perf_counter()\n",
        "cpu_time_img = t1 - t0\n",
        "print(f\"CPU invert time: {cpu_time_img:.4f} s\")\n",
        "\n",
        "# ---------------- GPU inversion ----------------\n",
        "arr_gpu = cp.asarray(arr)\n",
        "\n",
        "t0 = time.perf_counter()\n",
        "inv_gpu = 255 - arr_gpu\n",
        "cp.cuda.Stream.null.synchronize()\n",
        "t1 = time.perf_counter()\n",
        "gpu_time_img = t1 - t0\n",
        "print(f\"GPU invert time: {gpu_time_img:.4f} s\")\n",
        "\n",
        "# Copy result back\n",
        "inv_gpu_host = cp.asnumpy(inv_gpu)\n",
        "\n",
        "# ---------------- Verify ----------------\n",
        "identical = np.array_equal(inv_cpu, inv_gpu_host)\n",
        "print(\"Outputs identical:\", identical)\n",
        "\n",
        "# Save results\n",
        "Image.fromarray(inv_cpu).save(\"inverted_cpu.png\")\n",
        "Image.fromarray(inv_gpu_host).save(\"inverted_gpu.png\")\n",
        "print(\"Saved inverted_cpu.png and inverted_gpu.png\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
